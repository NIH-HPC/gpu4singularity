#!/bin/bash
# ~~~~The following text is automatically generated. Do not edit manually.~~~~
# ============================================================================
#                                                                             
#                            PUBLIC DOMAIN NOTICE                             
#                     Center for Information Technology (CIT)                 
#                                                                             
#  This software/database is a "United States Government Work" under the      
#  terms of the United States Copyright Act.  It was written as part of       
#  the author's official duties as a United States Government employee and    
#  thus cannot be copyrighted.  This software is freely available             
#  to the public for use.  The Center for Information Technology and the U.S. 
#  Government have not placed any restriction on its use or reproduction.     
#                                                                             
#  Although all reasonable efforts have been taken to ensure the accuracy     
#  and reliability of the software and data, CIT and the U.S.                 
#  Government do not and cannot warrant the performance or results that       
#  may be obtained by using this software or data. CIT and the U.S.           
#  Government disclaim all warranties, express or implied, including          
#  warranties of performance, merchantability or fitness for any particular   
#  purpose.                                                                   
#                                                                             
#  Please cite the author and the "NIH Biowulf Cluster" in any work or product
#  based on this material.                                                    
#                                                                             
#  Author:    David Godlove                                                      
#  Email:     godlovedc@helix.nih.gov                                            
#  Date:      Fri Sep 23 17:22:25 EDT 2016                                       
#                                                                            
#  Last edit: Ubuntu <ubuntu@ip-172-31-23-144.us-west-2.compute.internal>                                                           
#  On Date:   Sat Apr  8 04:16:54 UTC 2017                                                                   
#  Revision:  2.1.1                                                           
# ============================================================================
                                                                              
AUTH="David Godlove"
EMAIL="godlovedc@helix.nih.gov"
AUTH_DATE="Fri Sep 23 17:22:25 EDT 2016"
COMMIT_AUTH="Ubuntu <ubuntu@ip-172-31-23-144.us-west-2.compute.internal>"
COMMIT_DATE="Sat Apr  8 04:16:54 UTC 2017"
REV="2.1.1"
# ~~~~~~~~~~~~~~~~~~~~~~~~ Do not edit above this line.~~~~~~~~~~~~~~~~~~~~~~~

NVID_VER=367.48
export PERL5LIB=${PERL5LIB}:.

# these can be overridden with user input (below)
SRC_DIR=/tmp/CUDA.${NVID_VER}
BUILD_DIR=/tmp/CUDA.${NVID_VER}/build
DRY_RUN=N
VERBOSE=N
FROM_CUDA=""
RDL=""

FTP="ftp://helix.nih.gov/CUDA"

function _usage {
    me=$(basename $0)
    cat << EOF

    USAGE: $me  [ -h ] [ -V <nvidia_version> ] [ -v ] [ -n ] [ -d ] [ -N ] [ -u <full_url> ] [ -r <root_download_dir> ] [ -b <build_dir> ]

    DESCRIPTION:
        Installs or changes NVIDIA driver, within a Singularity container so
        that the container is able to utilize GPU nodes in the NIH HPC Biowulf
        cluster.  It does the following:

        1) Downloads NVIDIA .run, from the NIH HPC ftp site (These files are
           maintained to be compatible with current software versions on NIH
           HPC GPU nodes.)
           The downloaded file must match the following name format:
           NVIDIA-Linux-x86_64-<version>.run
        2) Extracts and copies binaries and creates links.
        3) Sets environment within the container to recognize driver binaries
           at runtime.

        To use it, shell into an existing Singularity container on your build
        system and issue the following commands or include them in your .def
        file.

        wget ${FTP}/gpu4singularity
        chmod 755 gpu4singularity
        ./gpu4singularity <options>
        rm gpu4singularity

    OPTIONS:
        -h, --help
                display this help message

        -b, --build-dir <build_dir>
                specify a temporory directory for downloading, extracting, and
                building files (to be deleted when finished)

        -v, --verbose
                echo commands before running them

        -n, --dry-run
                don\'t actually do anything

        -c, --from-cuda
                Extract the nvidia<version>.run file and install drivers 
                from a cuda<version>.run archive bundle.

        -u, --full-url
                The URL to look for the NVIDIA .run file. By default, it will
                use the one hosted at the NIH HPC ftp site.

        -r, --root-download-link
                If the full-url is not set, it will try to get the
                NVIDIA .run file at that Root URL directory,
                concatenated with the string NVIDIA-Linux-x86_64-<version>.run
                By default, it will use the file hosted at the NIH HPC ftp site.

        -V, --nvidia-version
                The version of the nvidia driver

        -d, --debug
                combines verbose and dry-run options

    AUTHORS:
        Written by:  ${AUTH}
        Email:       ${EMAIL}
        Date:        ${AUTH_DATE}

        Last edit:   ${COMMIT_AUTH}
        On Date:     ${COMMIT_DATE}
        Revision:    ${REV}

EOF
    exit 1
}


function _do_or_die {
    local cmd="$@"
    if [[ $VERBOSE == Y ]]; then
        echo gpu4singularity RUNNING: $cmd
    fi
    if [[ $DRY_RUN == N ]]; then
        $cmd
    fi
    if [ $? != 0 ]; then
        echo "ERROR: could not execute '${cmd}'"
        echo NVIDIA/CUDA installation failed.
        echo "***ABORTING***"
        exit 1
    fi
}


# parse any input
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
getopt --test > /dev/null
if [[ $? -ne 4 ]]; then
    echo "I'm sorry, `getopt --test` failed in this environment."
    echo "gpu4singularity cannot parse arguments with getopt on your build system."
    exit 1
fi

SHORT=h,n,v,d,c,u:,r:,V:,b:
LONG=help,dry-run,verbose,debug,from-cuda,full-url:,root-download-link:,nvidia-version:,build-dir:

# -temporarily store output to be able to check for errors
# -activate advanced mode getopt quoting e.g. via "--options"
# -pass arguments only via   -- "$@"   to separate them correctly
PARSED=`getopt --options $SHORT --longoptions $LONG --name "$0" -- "$@"`
if [[ $? -ne 0 ]]; then
    # e.g. $? == 1
    #  then getopt has complained about wrong arguments to stdout
    exit 2
fi
# use eval with "$PARSED" to properly handle the quoting
eval set -- "$PARSED"

# now enjoy the options in order and nicely split until we see --
while true; do
    case "$1" in
        -h|--help)
            _usage
            shift
            ;;
        -n|--dry-run)
            echo "dry-run: no changes will be made"
            DRY_RUN=Y
            shift
            ;;
        -v|--verbose)
            echo "verbose: running with extra output"
            VERBOSE=Y
            shift
            ;;
        -d|--debug)
            echo "debug mode: dry-run + verbose"
            DRY_RUN=Y
            VERBOSE=Y
            shift
            ;;
        -c|--from-cuda)
            FROM_CUDA=N
            shift
            ;;
        -u|--full-url)
            URL="$2"
            shift 2
            ;;
        -r|--root-download-link)
            RDL="$2"
            shift 2
            ;;
        -V|--nvidia-version)
            NVID_VER="$2"
            shift 2
            ;;
        -b|--build-dir)
            BUILD_DIR="$2"
            shift 2
            ;;
        --)
            shift
            break
            ;;
        *)
            _usage
            echo "Programming error !! getopt can not parse your options."
            exit 3
            ;;
    esac
done

# handle non-option arguments
if [[ $# -ne 0 ]]; then
    echo "$0: unknown option(s)"
    _usage
fi


# Check that container has dependencies
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_do_or_die "mkdir -p ${BUILD_DIR}"
_do_or_die "cd ${SRC_DIR}"

echo ""
echo "Looking for dependencies in container..."
_do_or_die "which which"
_do_or_die "which bash"
_do_or_die "which tar"
_do_or_die "which gzip"
_do_or_die "which wget"
_do_or_die "which perl"
echo ""


# Download and install libraries and drivers
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# download necessary files into image
if [[ -z "$URL" ]];then
  if [[ -n "$RDL" ]]; then
    URL=$RDL"/NVIDIA-Linux-x86_64-"$NVID_VER".run"
  else
    URL=$FTP"/NVIDIA-Linux-x86_64-"$NVID_VER".run"
  fi
fi

_do_or_die "wget ${URL}"
_do_or_die "chmod 755 ${SRC_DIR}/*"
# extract nvidia files
if [[ $FROM_CUDA == N ]];then
  _do_or_die "bash ./*${NVID_VER}* -extract=${BUILD_DIR}"
  _do_or_die "${BUILD_DIR}/NVIDIA-Linux-x86_64-${NVID_VER}.run --extract-only"
else
  _do_or_die "${SRC_DIR}/NVIDIA-Linux-x86_64-${NVID_VER}.run --extract-only"
fi



# make links in nvidia directory
_do_or_die "cd NVIDIA-Linux-x86_64-${NVID_VER}"
_do_or_die "ln -s libEGL_nvidia.so.${NVID_VER}         libEGL_nvidia.so.0"
_do_or_die "ln -s libGLESv1_CM_nvidia.so.${NVID_VER}   libGLESv1_CM_nvidia.so.1"
_do_or_die "ln -s libGLESv2_nvidia.so.${NVID_VER}      libGLESv2_nvidia.so.2"
_do_or_die "ln -s libGLX_nvidia.so.${NVID_VER}         libGLX_indirect.so.0"
_do_or_die "ln -s libGLX_nvidia.so.${NVID_VER}         libGLX_nvidia.so.0"
_do_or_die "ln -s libnvidia-cfg.so.1                   libnvidia-cfg.so"
_do_or_die "ln -s libnvidia-cfg.so.${NVID_VER}         libnvidia-cfg.so.1"
_do_or_die "ln -s libnvidia-encode.so.1                libnvidia-encode.so"
_do_or_die "ln -s libnvidia-encode.so.${NVID_VER}      libnvidia-encode.so.1"
_do_or_die "ln -s libnvidia-fbc.so.1                   libnvidia-fbc.so"
_do_or_die "ln -s libnvidia-fbc.so.${NVID_VER}         libnvidia-fbc.so.1"
_do_or_die "ln -s libnvidia-ifr.so.1                   libnvidia-ifr.so"
_do_or_die "ln -s libnvidia-ifr.so.${NVID_VER}         libnvidia-ifr.so.1"
_do_or_die "ln -s libnvidia-ml.so.1                    libnvidia-ml.so"
_do_or_die "ln -s libnvidia-ml.so.${NVID_VER}          libnvidia-ml.so.1"
_do_or_die "ln -s libnvidia-opencl.so.${NVID_VER}      libnvidia-opencl.so.1"
_do_or_die "ln -s vdpau/libvdpau_nvidia.so.${NVID_VER} libvdpau_nvidia.so"
_do_or_die "ln -s libcuda.so.${NVID_VER}               libcuda.so"
_do_or_die "ln -s libcuda.so.${NVID_VER}               libcuda.so.1"

# move into place (overwrite old if exist)
_do_or_die "cd .."
_do_or_die "rm -rf /usr/local/nvidia"
_do_or_die "mv NVIDIA-Linux-x86_64-${NVID_VER} /usr/local/nvidia"


# set up /environment to use nvidia drivers and cuda
# changing ./tmp to ./tmp_env to avoid any confusion with /tmp
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
if [[ $VERBOSE == Y ]]; then
    echo ""
    echo "Adding environmental variables to ./tmp_env"
fi

# 1) if placeholder tags don't exist, make them
if [[ $DRY_RUN == N ]]; then
    tag1='### START NVIDIA PATHS'
    tag2='### END NVIDIA PATHS'
    findtext=$( grep "${tag1}" /environment )
    if [ -z "${findtext}" ]; then
        cat >> /environment <<END_OF_SCRIPT

${tag1}
${tag2}

END_OF_SCRIPT
    fi

    # 2) cat /environment (before, in, and after placeholder tags) to ./tmp
    grep -B10000 "${tag1}" /environment > tmp_env
    cat >> tmp_env <<END_OF_SCRIPT
PATH=\$PATH:/usr/local/nvidia:/usr/local/cuda-${NVID_VER}/bin
LD_LIBRARY_PATH=\$LD_LIBRARY_PATH:/usr/local/nvidia
export PATH LD_LIBRARY_PATH
END_OF_SCRIPT
    grep -A10000 "${tag2}" /environment >> tmp_env
fi

# 3) move into place
_do_or_die "mv ./tmp_env /environment"


# cleanup
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
_do_or_die "cd .."
_do_or_die "rm -rf ${BUILD_DIR}"
